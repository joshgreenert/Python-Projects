{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fca2ca",
   "metadata": {},
   "source": [
    "# Data Preparation Week 9 and 10\n",
    "## Joshua Greenert\n",
    "## DSC540-T301 Data Preparation\n",
    "## 10/28/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeab84e",
   "metadata": {},
   "source": [
    "## Data Wrangling with Python Activity 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d017a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ssl\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7893421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SSL cert.\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30ca1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HTML from the url\n",
    "top100url = 'https://www.gutenberg.org/browse/scores/top'\n",
    "response = requests.get(top100url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99f2364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Write a small function to check the status of the web request.\n",
    "def check_status(response):\n",
    "    if response.status_code == 200:\n",
    "        print(\"Success!\")\n",
    "    else:\n",
    "        print(\"Failed!\")\n",
    "\n",
    "check_status(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ff5b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the response and pass this on to Beautiful Soup for HTML parsing.\n",
    "content = response.content.decode(response.encoding)\n",
    "soup = BeautifulSoup(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbd5b9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/about/',\n",
       " '/about/',\n",
       " '/policy/collection_development.html',\n",
       " '/about/contact_information.html',\n",
       " '/about/background/',\n",
       " '/policy/permission.html',\n",
       " '/policy/privacy_policy.html',\n",
       " '/policy/terms_of_use.html',\n",
       " '/ebooks/',\n",
       " '/ebooks/',\n",
       " '/ebooks/bookshelf/',\n",
       " '/browse/scores/top',\n",
       " '/ebooks/offline_catalogs.html',\n",
       " '/help/',\n",
       " '/help/',\n",
       " '/help/copyright.html',\n",
       " '/help/errata.html',\n",
       " '/help/file_formats.html',\n",
       " '/help/faq.html',\n",
       " '/policy/',\n",
       " '/help/public_domain_ebook_submission.html',\n",
       " '/help/submitting_your_own_work.html',\n",
       " '/help/mobile.html',\n",
       " '/attic/',\n",
       " '/donate/',\n",
       " '/donate/',\n",
       " '#books-last1',\n",
       " '#authors-last1',\n",
       " '#books-last7']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the href tags and store them in the list of links.  Print the first 30 elements.\n",
    "href_links = []\n",
    "\n",
    "# Loop through the soup to get the links.\n",
    "for i in soup.find_all('a'):\n",
    "    href_links.append(i.get('href'))\n",
    "    \n",
    "href_links[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f65a7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a regular expression to find the numeric digits in the links.  These are the file number for top 100 ebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "861d7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the empty list to hold the file numbers and use regex to find the numeric digits in the link href string.\n",
    "# Use the findall method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the soup object's text look like?  Use the .text method and print only the first 2,000 characters \n",
    "# (not the whole string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67279bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in the extracted text from the soup object to find the names of the top 100 ebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a starting index.  It should point at the text top 100 ebooks yesterday.  Use the splitlines method of soup.text\n",
    "# It splits the lines of text of the soup object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "857ef4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop 1 - 100 to add the strings of the next 100 lines to this temporary list.\n",
    "# Hint: use the splitlines method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c58eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a regular expression to extract only text from the name strings and append it to an empty list.\n",
    "# Use match and span to find the indcies and use them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97c3d6",
   "metadata": {},
   "source": [
    "## Data Wrangling with Python Activity 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4ca4fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the secret API key using json.loads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8c4867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a key and store it in json as apikesy.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f62cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the APIkeys.json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb2efb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the OMDB portal (http://www.omdbapi.com/?) as a string to a variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12d43439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable called apikey with the last portion of the URL\n",
    "# &apikey = secret apikey where secretapikey is your api key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3914cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a utility function called print_json to print th emovie data from a json file (from the portal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a utility function to download a poster of the movie based on the information from the json dataset and save it\n",
    "# in your local folder.  Use the os module.  The poster data is stored in the json key poster.  Use the python command\n",
    "# to open a file and write the poster data.  Close the file after you're done (save as image file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a07cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a utility function called search_movie to search for a movie by its name, print the downloaded json data, and save\n",
    "# the movie poster in the local folder.  Use a try-except loop for this.  Use the previously created serviceurl and apikey\n",
    "# variables.  You have to pass on a dictionary with a key, t, and the movie name as the corresponding value to the \n",
    "# urlencode() function and then add the serviceurl and apikey to the output of the function to construct the full url.  This\n",
    "# url will be used to access the data.  The json data has a key called response.  If it is true, that means the read was\n",
    "# successful.  Check this before processing the data.  If it's not successful, print the json key error, which will contain\n",
    "# the appropriate error message returned by the movie database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the search_movie function by entering Titanic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the search_movie function by entering \"Random_error\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff8497",
   "metadata": {},
   "source": [
    "## Connect to Twitter Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0157095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author ID: 1448591917076332548\n",
      "Text: RT @parvezshahshaik: Python Libraries and Frameworks\n",
      "#ArtificialIntelligence #AI #ML #DataScience #DataScientists #CodeNewbies #Tech #deeplâ€¦\n",
      "\n",
      "Author ID: 1202221488\n",
      "Text: HIRING: Enterprise Major Account Manager / USA - Remote https://t.co/j73CYF4PE4 #FRW #remote #remotework #remotejobs #jobalert #wfh #workfromhome #hiringnow #jobsearch #RemoteJob #Analytics #B2B #BigData #DataScience #Interpersonalskills #MachineLearning #OpenSource\n",
      "\n",
      "Author ID: 97979745\n",
      "Text: RT @Sheraj99: 7 Steps #MachineLearning #DataScience #SQL #Cybersecurity #BigData #Analytics #AI #IIoT #Python #RStats #TensorFlow #JavaScriâ€¦\n",
      "\n",
      "Author ID: 1448591917076332548\n",
      "Text: RT @_qu4nt: #Infographic: #FullStack Developer skills\n",
      "Via @ingliguori\n",
      "\n",
      "#MachineLearning #DataScience #SQL #Cybersecurity #BigData #Analyticâ€¦\n",
      "\n",
      "Author ID: 135145900\n",
      "Text: RT @CodingPseudo: Computer Vision in Python \n",
      "\n",
      "https://t.co/38b3HqhwdZ via @YouTube \n",
      "\n",
      "#computer #vision #datascience #artificialintelligenceâ€¦\n",
      "\n",
      "Author ID: 1185559720339394561\n",
      "Text: RT @LegitWriters1: HMU for quality assignment help\n",
      "Essay pay\n",
      "Exams\n",
      "Homework.\n",
      "Biology\n",
      "Chemistry\n",
      "Math\n",
      "Full classes\n",
      "Ecology.\n",
      "Anatomy.\n",
      "#Machineâ€¦\n",
      "\n",
      "Author ID: 1006855306280828928\n",
      "Text: RT @Sheraj99: How Much Free #Cloud Storage You can have #MachineLearning #DataScience #SQL #Cybersecurity #BigData #Analytics #AI #IIoT #Pyâ€¦\n",
      "\n",
      "Author ID: 1408718149835501568\n",
      "Text: HMU for quality assignment help\n",
      "Essay pay\n",
      "Homework\n",
      "Chemistry\n",
      "Math,..\n",
      "Full classes.,\n",
      "Ecology\n",
      "Anatomy\n",
      "#MachineLearning  #DataScience #5G #100DaysOfCode\n",
      "#Python #Cybersecurity #BigData #AI #IoT #DeepLearning\n",
      "#ArtificialIntelligence #NLP #robots  #javascript\n",
      "\n",
      "DM @_paperhelp https://t.co/HcndbD5CQz\n",
      "\n",
      "Author ID: 1511954357138636800\n",
      "Text: RT @LegitWriters1: HMU for quality assignment help\n",
      "Essay pay\n",
      "Exams\n",
      "Homework.\n",
      "Biology\n",
      "Chemistry\n",
      "Math\n",
      "Full classes\n",
      "Ecology.\n",
      "Anatomy.\n",
      "#Machineâ€¦\n",
      "\n",
      "Author ID: 971620109197312000\n",
      "Text: RT @LegitWriters1: HMU for quality assignment help\n",
      "Essay pay\n",
      "Exams\n",
      "Homework.\n",
      "Biology\n",
      "Chemistry\n",
      "Math\n",
      "Full classes\n",
      "Ecology.\n",
      "Anatomy.\n",
      "#Machineâ€¦\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set up the api connection\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAH3HigEAAAAApWixubJom%2BYZEQQLNmjlPP7mMXE%3Drfuw5ezx3wgBGUeqHlYRXv3LLlVGb5fKOPhll0BdG3iJOTrGm0\"\n",
    "\n",
    "# Set up url and query parameters.\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "query_params = {'query': '(from:twitterdev -is:retweet) OR #bellevue OR #datascience','tweet.fields': 'author_id'}\n",
    "\n",
    "# Set the headers for the bearer token.\n",
    "def bearer_oauth(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "# Connect to the search endpoint and to return data.\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "# Get the response.\n",
    "json_response = connect_to_endpoint(search_url, query_params)\n",
    "json_data = json.dumps(json_response)\n",
    "\n",
    "# Parse the response and print the author id and text.\n",
    "for val in json_response[\"data\"]:\n",
    "    print(f'\\nAuthor ID: {val[\"author_id\"]}\\nText: {val[\"text\"].strip()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e23ec",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6faf7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one of the datasets provided in Weeks 7 & 8, or a dataset of your own, choose 3 of the following visualizations \n",
    "# to complete. You must submit via PDF along with your code. You are free to use Matplotlib, Seaborn or another package \n",
    "# if you prefer. (line, bar, scatter, histogram, density plot, pie chart)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
